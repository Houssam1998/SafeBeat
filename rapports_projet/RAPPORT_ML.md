# ü§ñ SafeBeat - Rapport D√©taill√© Machine Learning

<center>

## **Mod√®les d'Apprentissage Automatique**
### Documentation Technique et R√©sultats

</center>

---

## üìã Table des Mati√®res

1. [Vue d'Ensemble des Mod√®les](#1-vue-densemble-des-mod√®les)
2. [Association Rules Mining](#2-association-rules-mining)
3. [K-Means Clustering](#3-k-means-clustering)
4. [Time Series Forecasting](#4-time-series-forecasting)
5. [Random Forest Classification](#5-random-forest-classification)
6. [KNN Classification](#6-knn-classification)
7. [Int√©gration Production](#7-int√©gration-production)

---

## 1. Vue d'Ensemble des Mod√®les

### 1.1 Portfolio ML

| Mod√®le | Type | Objectif | Accuracy/Score |
|--------|------|----------|----------------|
| **Apriori** | Unsupervised | Pattern Discovery | 24 rules, Lift >1.5 |
| **K-Means** | Unsupervised | Zone Segmentation | Silhouette: 0.42 |
| **Ridge/GB** | Regression | Workload Forecast | MAE: 45.2, R¬≤: 0.78 |
| **Random Forest** | Classification | Priority Prediction | Accuracy: 61% |
| **KNN** | Classification | Incident Type | Accuracy: 58% |

### 1.2 Architecture ML Pipeline

```mermaid
flowchart TB
    subgraph Data["üìä Data Sources"]
        DW[("PostgreSQL<br/>fact_911_calls")]
        MINIO[("MinIO<br/>Enriched Data")]
    end
    
    subgraph Training["üéì Training Phase"]
        T1["Association Rules"]
        T2["K-Means"]
        T3["Time Series"]
        T4["Random Forest"]
    end
    
    subgraph Models["üì¶ Saved Models"]
        M1["association_rules_results.csv"]
        M2["kmeans_model.pkl"]
        M3["timeseries_model.pkl"]
        M4["rf_priority_model.pkl"]
    end
    
    subgraph Inference["üîÆ Production Inference"]
        I1["Pattern Alerts"]
        I2["Zone Risk Map"]
        I3["Workload Forecast"]
        I4["Priority Scores"]
    end
    
    Data --> Training
    Training --> Models
    Models --> Inference
```

---

## 2. Association Rules Mining

### 2.1 Algorithme: Apriori

**Objectif**: D√©couvrir les associations fr√©quentes entre conditions et types d'incidents.

**Formulation Math√©matique**:
```
Support(A‚ÜíB) = P(A ‚à© B)
Confidence(A‚ÜíB) = P(B|A) = Support(A‚à©B) / Support(A)
Lift(A‚ÜíB) = Confidence(A‚ÜíB) / Support(B)
```

### 2.2 Impl√©mentation

```python
# models/association_rules.py
from mlxtend.frequent_patterns import apriori, association_rules
from mlxtend.preprocessing import TransactionEncoder

def run_association_rules():
    # 1. Prepare transactions
    df = load_911_data()
    
    # Create itemsets from categorical columns
    transactions = df.apply(lambda row: [
        f"priority_{row['priority_level']}",
        f"hour_{row['response_hour'] // 6}",  # 4 time buckets
        f"day_{row['response_day_of_week']}",
        f"category_{row['final_problem_category'][:20]}"
    ], axis=1).tolist()
    
    # 2. One-hot encoding
    te = TransactionEncoder()
    te_ary = te.fit_transform(transactions)
    df_encoded = pd.DataFrame(te_ary, columns=te.columns_)
    
    # 3. Find frequent itemsets
    frequent_itemsets = apriori(
        df_encoded, 
        min_support=0.01,  # 1% minimum
        use_colnames=True
    )
    
    # 4. Generate rules
    rules = association_rules(
        frequent_itemsets,
        metric="lift",
        min_threshold=1.2  # Lift > 1.2
    )
    
    return rules
```

### 2.3 R√©sultats

| R√®gle | Support | Confidence | Lift |
|-------|---------|------------|------|
| {Weekend, Night} ‚Üí {Disturbance} | 0.045 | 0.67 | 2.3 |
| {Downtown, Night} ‚Üí {Assault} | 0.032 | 0.52 | 1.9 |
| {Clear Weather} ‚Üí {Traffic} | 0.078 | 0.41 | 1.5 |
| {Festival Period} ‚Üí {Intoxication} | 0.025 | 0.71 | 2.8 |

**Total Rules Discovered**: 24

### 2.4 Visualisation

```python
# Scatter plot Support vs Confidence
fig = px.scatter(
    rules,
    x='support',
    y='confidence',
    size='lift',
    color='lift',
    hover_data=['antecedents', 'consequents'],
    title='Association Rules - Support vs Confidence'
)
```

---

## 3. K-Means Clustering

### 3.1 Algorithme

**Objectif**: Segmenter les zones g√©ographiques en clusters de risque.

**Formulation Math√©matique**:
```
J = Œ£·µ¢ Œ£‚Çì‚ààC·µ¢ ||x - Œº·µ¢||¬≤

o√π:
- J = Within-cluster sum of squares (WCSS)
- C·µ¢ = Cluster i
- Œº·µ¢ = Centroid du cluster i
```

### 3.2 Impl√©mentation

```python
# models/kmeans_clustering.py
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

def train_kmeans():
    # 1. Load geo-enriched data
    df = load_enriched_911()
    
    # 2. Aggregate by location
    geo_stats = df.groupby(['latitude_centroid', 'longitude_centroid']).agg({
        'incident_number': 'count',
        'priority_numeric': 'mean'
    }).reset_index()
    
    geo_stats.columns = ['lat', 'lon', 'incident_count', 'avg_priority']
    
    # 3. Feature engineering
    features = geo_stats[['lat', 'lon', 'incident_count']]
    
    # 4. Scaling
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(features)
    
    # 5. Find optimal K (Elbow method)
    wcss = []
    for k in range(2, 10):
        kmeans = KMeans(n_clusters=k, random_state=42)
        kmeans.fit(X_scaled)
        wcss.append(kmeans.inertia_)
    
    # K=3 optimal based on elbow
    optimal_k = 3
    
    # 6. Final model
    model = KMeans(n_clusters=optimal_k, random_state=42)
    geo_stats['cluster'] = model.fit_predict(X_scaled)
    
    # 7. Label clusters
    cluster_means = geo_stats.groupby('cluster')['incident_count'].mean()
    risk_labels = cluster_means.rank(ascending=False).map({
        1: 'HIGH_RISK',
        2: 'MEDIUM_RISK',
        3: 'LOW_RISK'
    })
    
    geo_stats['risk_zone'] = geo_stats['cluster'].map(risk_labels)
    
    return model, geo_stats
```

### 3.3 R√©sultats

| Cluster | Label | Zones | Avg Incidents | Characteristics |
|---------|-------|-------|---------------|-----------------|
| 0 | HIGH_RISK | 127 | 1,247 | Downtown, 6th Street |
| 1 | MEDIUM_RISK | 342 | 456 | East Austin, University |
| 2 | LOW_RISK | 297 | 89 | Residential areas |

**Silhouette Score**: 0.42 (bon clustering)

### 3.4 Visualisation G√©ographique

```python
fig = px.scatter_mapbox(
    clusters,
    lat='latitude',
    lon='longitude',
    color='risk_zone',
    size='incident_count',
    color_discrete_map={
        'HIGH_RISK': '#ff4b4b',
        'MEDIUM_RISK': '#ffa500',
        'LOW_RISK': '#00cc00'
    },
    mapbox_style='carto-positron',
    zoom=10
)
```

---

## 4. Time Series Forecasting

### 4.1 Algorithme

**Objectif**: Pr√©dire le volume quotidien d'appels 911.

**Mod√®les test√©s**:
- Ridge Regression
- Gradient Boosting Regressor

### 4.2 Feature Engineering

```python
# 12 features pour le mod√®le
features = [
    # Temporal (3)
    'day_of_week',      # 0-6
    'month',            # 1-12
    'is_weekend',       # 0/1
    
    # Cyclical encoding (4)
    'dow_sin',          # sin(2œÄ * dow / 7)
    'dow_cos',          # cos(2œÄ * dow / 7)
    'month_sin',        # sin(2œÄ * month / 12)
    'month_cos',        # cos(2œÄ * month / 12)
    
    # Lag features (3)
    'call_count_lag_1',   # Hier
    'call_count_lag_7',   # Semaine derni√®re
    'call_count_lag_14',  # 2 semaines
    
    # Rolling features (2)
    'call_count_rolling_mean_7',   # Moyenne 7 jours
    'call_count_rolling_mean_14',  # Moyenne 14 jours
]
```

### 4.3 Impl√©mentation

```python
# models/timeseries_forecast.py
from sklearn.linear_model import Ridge
from sklearn.ensemble import GradientBoostingRegressor

def forecast_workload():
    # 1. Aggregate daily counts
    daily = df.groupby('response_date').size().reset_index()
    daily.columns = ['date', 'call_count']
    
    # 2. Create features
    daily['day_of_week'] = daily['date'].dt.dayofweek
    daily['month'] = daily['date'].dt.month
    daily['is_weekend'] = (daily['day_of_week'] >= 5).astype(int)
    
    # Cyclical
    daily['dow_sin'] = np.sin(2 * np.pi * daily['day_of_week'] / 7)
    daily['dow_cos'] = np.cos(2 * np.pi * daily['day_of_week'] / 7)
    
    # Lag features
    for lag in [1, 7, 14, 28]:
        daily[f'call_count_lag_{lag}'] = daily['call_count'].shift(lag)
    
    # Rolling features
    for window in [7, 14]:
        daily[f'call_count_rolling_mean_{window}'] = \
            daily['call_count'].rolling(window).mean()
    
    # 3. Train/Test split (time-based 80/20)
    split_idx = int(len(daily) * 0.8)
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]
    
    # 4. Train models
    ridge = Ridge(alpha=1.0)
    ridge.fit(X_train, y_train)
    
    gb = GradientBoostingRegressor(n_estimators=100)
    gb.fit(X_train, y_train)
    
    # 5. Evaluate
    # Ridge: MAE=52.3, R¬≤=0.71
    # GB: MAE=45.2, R¬≤=0.78 ‚Üê Best
    
    return best_model
```

### 4.4 R√©sultats

| Mod√®le | MAE | RMSE | R¬≤ | MAPE |
|--------|-----|------|-----|------|
| Ridge | 52.3 | 67.8 | 0.71 | 7.1% |
| **Gradient Boosting** | **45.2** | **58.4** | **0.78** | **5.8%** |

**Forecast Moyen**: 784 calls/day

### 4.5 Feature Importance (GB)

| Feature | Importance |
|---------|------------|
| call_count_lag_1 | 0.342 |
| call_count_rolling_mean_7 | 0.218 |
| call_count_lag_7 | 0.156 |
| day_of_week | 0.098 |
| is_weekend | 0.072 |

---

## 5. Random Forest Classification

### 5.1 Algorithme

**Objectif**: Pr√©dire la priorit√© d'un incident pour optimiser l'allocation des ressources.

### 5.2 Impl√©mentation

```python
# models/random_forest_priority.py
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

def train_priority_classifier():
    # 1. Prepare features
    features = [
        'response_hour',
        'response_day_of_week',
        'response_month',
        'latitude_centroid',
        'longitude_centroid',
        'problem_category_encoded'  # LabelEncoded
    ]
    
    X = df[features]
    y = df['priority_class']  # 0-4
    
    # 2. Handle class imbalance
    from sklearn.utils.class_weight import compute_class_weight
    class_weights = compute_class_weight('balanced', ...)
    
    # 3. Train/Test split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, stratify=y, random_state=42
    )
    
    # 4. Train model
    model = RandomForestClassifier(
        n_estimators=100,
        max_depth=15,
        class_weight='balanced',
        random_state=42
    )
    model.fit(X_train, y_train)
    
    # 5. Evaluate
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)  # 0.61
    
    return model
```

### 5.3 R√©sultats

**Accuracy Globale**: 61%

| Priority | Precision | Recall | F1-Score | Support |
|----------|-----------|--------|----------|---------|
| 0 (Critical) | 0.45 | 0.38 | 0.41 | 2,104 |
| 1 (Emergency) | 0.52 | 0.47 | 0.49 | 18,521 |
| 2 (Urgent) | 0.68 | 0.72 | 0.70 | 89,456 |
| 3 (Moderate) | 0.55 | 0.52 | 0.53 | 42,178 |
| 4 (Low) | 0.48 | 0.41 | 0.44 | 14,781 |

### 5.4 Feature Importance

| Feature | Importance |
|---------|------------|
| problem_category_encoded | 0.42 |
| response_hour | 0.15 |
| latitude_centroid | 0.12 |
| longitude_centroid | 0.11 |
| response_day_of_week | 0.10 |
| response_month | 0.10 |

---

## 6. KNN Classification

### 6.1 Algorithme: K-Nearest Neighbors

**Objectif**: Classifier le type d'incident.

### 6.2 Impl√©mentation

```python
from sklearn.neighbors import KNeighborsClassifier

def train_knn():
    # Features
    X = df[['response_hour', 'response_day_of_week', 
            'latitude_centroid', 'longitude_centroid']]
    y = df['incident_type_category']  # Grouped categories
    
    # Optimal K via cross-validation
    best_k = 7
    
    model = KNeighborsClassifier(n_neighbors=best_k, weights='distance')
    model.fit(X_train, y_train)
    
    # Accuracy: 58%
    return model
```

### 6.3 R√©sultats

**Accuracy**: 58%

| K | Accuracy |
|---|----------|
| 3 | 0.52 |
| 5 | 0.56 |
| **7** | **0.58** |
| 9 | 0.57 |
| 11 | 0.55 |

---

## 7. Int√©gration Production

### 7.1 Sauvegarde des Mod√®les

```python
# Save to MinIO models/trained/
import pickle

model_data = {
    'model': trained_model,
    'model_name': 'RandomForest',
    'scaler': scaler,
    'features': feature_list,
    'metrics': {'accuracy': 0.61}
}

with open('rf_priority_model.pkl', 'wb') as f:
    pickle.dump(model_data, f)

# Upload to MinIO
client.put_object('models', 'trained/rf_priority_model.pkl', ...)
```

### 7.2 Inference en Production

```python
def run_forecasting(**context):
    # Load model from MinIO
    model_dict = load_model_from_minio('timeseries_model.pkl')
    model = model_dict['model']
    scaler = model_dict['scaler']
    features = model_dict['features']
    
    # Prepare features matching training
    X_future = prepare_features(future_dates)
    X_scaled = scaler.transform(X_future[features])
    
    # Predict
    predictions = model.predict(X_scaled)
    
    # Save results
    save_results_to_minio(predictions, 'ml_results/forecast.parquet')
```

### 7.3 Pre-checks Production

```python
# Before running each model
cursor.execute("SELECT COUNT(*) FROM fact_911_calls")
record_count = cursor.fetchone()[0]

if record_count < MIN_RECORDS:
    print(f"‚ö†Ô∏è PRE-CHECK FAILED: {record_count:,} records")
    return False

print(f"‚úÖ PRE-CHECK PASSED: {record_count:,} records")
```

---

## üìä R√©sum√© des Performances

| Mod√®le | M√©trique Principale | Valeur |
|--------|---------------------|--------|
| Association Rules | Rules Discovered | 24 |
| K-Means | Silhouette Score | 0.42 |
| Time Series | R¬≤ | 0.78 |
| Random Forest | Accuracy | 61% |
| KNN | Accuracy | 58% |

---

*Document g√©n√©r√© automatiquement - SafeBeat ML v2.0*
